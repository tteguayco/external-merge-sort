# External merge sort

## Install dependencies

Dependencies for the project are listed inside the ```requirements.txt``` file and can be installed via pip from the root directory:

```pip install -r ./requirements.txt```

## Project structure

The project is organized mainly in source code files located in ```src``` and some unit tests in ```tests```.

In the root of the ```src``` folder, one file per exercise can be found: ```a.py```, ```b.py``` and ```c.py```.

Input (unsorted) file and output (sorted) files are read and written from and to ```src/data```.

External memory is simulated on ```src/external```.

**Note:** Execution of ```b.py``` takes around 20 minutes for an input file containing ```10_000_000``` integers.

## Run the tests

Tests can be run via the following command from the ```tests``` folder:

```python -m unittest```

## Answers to proposed questions

1. **This program is likely to be I/O bound. Is there any way to take advantage of this?**

I/O bound programs are related to how fast access to the secondary memory is. The only advantange I can think of right now would be to have a very fast disk, like a good SSD.

2. **Are there parts of the program that can be parallelized across multiple cores in the same machine?**

Both phases of the external merge sort algorithm, initial sort of chunks and later merge, can be parallelized.

The first phase can be paralellized in a way that each chunk is sorted in parallel by, for example, a thread each.

Also, the second phase where the "merge" part takes place can be parallelized. To achieve this, one alternative could be to distribute all the intermediate files from the first phase in as many groups as threads, and then apply *k-way sorting* on the files generated by all of these threads to get a final sorted file.

3. **Across multiple disks in the same machine?**

If our algorithm runs in parallel, it would be nice to distribute the access to the secondary memory in a uniform way across all disks we could have (if our machine has more than one).

4. **Across multiple machines?**

We could distribute the parallel execution of the algorithm across machines with a similar strategy described in 2. For both distribution of workload and join of results we could study how to adapt the problem to the MapReduce framework, for example.

5. **How would one choose M for different N, cores, spindles, and machines?**

In the case of cores and disks (spindles), M would depend on the RAM capacity of the concrete machine. In a machine-distributed approach, it would be nice to choose a machine-dependent value for M instead of a fixed value for all machines (unless all machines have the very same amount of RAM).
